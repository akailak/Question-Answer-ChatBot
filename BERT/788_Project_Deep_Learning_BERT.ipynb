{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mouting google drive to import data saved on it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyHa0lsoZxcp",
        "outputId": "beb54126-aa8d-47ee-822b-14fe866c0130"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK-NMEK7sb_k",
        "outputId": "7acfcd89-2c2d-441d-977d-0a7cee7f4671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "#installing the transformers package needed for NLP tasks involving transofrmers such as BERT\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cxz_DvI_xaFP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import json\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZH2dnS8lxa4r"
      },
      "outputs": [],
      "source": [
        "# loading the question-answer pairs file\n",
        "s08 = pd.read_csv('/content/QA Pairs.txt', sep='\\t', encoding = 'ISO-8859-1')\n",
        "# loading contexts for the above pairs\n",
        "contexts = pd.read_csv('/content/Context.txt', sep='\\n', encoding = 'ISO-8859-1', header=None)\n",
        "# loading the testing questions (anonymous question not in the dataset) and their answers and context\n",
        "test_set = pd.read_csv('/content/test2.txt', sep='\\t', encoding = 'ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bu5RAeolxdBV"
      },
      "outputs": [],
      "source": [
        "# making a new variable to manipulate the data while keeping the original\n",
        "trn = s08"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-vlhgtoMxfrW"
      },
      "outputs": [],
      "source": [
        "# dropping columns which we will not use and removing any NAN values then resetting the index for each set of data\n",
        "trn = trn.drop(columns = ['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleTitle'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rmKyt19exXSY"
      },
      "outputs": [],
      "source": [
        "# randomly selecting samples from the train set for validation\n",
        "val= trn.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-BhIcHY9T82D"
      },
      "outputs": [],
      "source": [
        "# getting the validation contexts of the samples that were selected\n",
        "val_contexts= []\n",
        "for i in range(len(val)):\n",
        "  val_contexts.append(contexts[0][val.index[i]])\n",
        "\n",
        "# storing the train contexts in a list instead of a dataframe\n",
        "train_contexts= []\n",
        "for i in range(len(contexts)):\n",
        "  train_contexts.append(contexts[0][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vY78zprJMV5K"
      },
      "outputs": [],
      "source": [
        "# resetting the indices of validation and training sets\n",
        "val = val.reset_index(drop=True)\n",
        "trn = trn.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2CwektSzr2cr"
      },
      "outputs": [],
      "source": [
        "# preparing train & validation questoins\n",
        "train_questions = trn['Question'].values.tolist()\n",
        "val_questions = val['Question'].values.tolist()\n",
        "\n",
        "# storing the test questions and contexts in lists\n",
        "test_questions= test_set['Question'].values.tolist()\n",
        "test_contexts= test_set['Context'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "afpm51ZwOWeD"
      },
      "outputs": [],
      "source": [
        "# preparing the train answers in json format (with the answer's start/end positions in their contexts)\n",
        "train_answers = []\n",
        "for i in range(len(trn)):\n",
        "  src_str = train_contexts[i]\n",
        "  start_index = src_str.find(trn['Answer'][i])\n",
        "  end_index = start_index + len(trn['Answer'][i])\n",
        "  train_answers.append(str(\"{\\\"text\\\": \\\"\" + trn['Answer'][i] + str(\"\\\", \\\"answer_start\\\": \") + \n",
        "                           str(start_index) + str(\", \\\"answer_end\\\": \") + str(end_index) + str(\"}\")))\n",
        "  \n",
        "# preparing the validation answers in json format (with the answer's start/end positions in their contexts)\n",
        "val_answers = []\n",
        "for i in range(len(val)):\n",
        "  src_str = val_contexts[i]\n",
        "  start_index = src_str.find(val['Answer'][i])\n",
        "  end_index = start_index + len(val['Answer'][i])\n",
        "  val_answers.append(str(\"{\\\"text\\\": \\\"\" + val['Answer'][i] + str(\"\\\", \\\"answer_start\\\": \") + \n",
        "                           str(start_index) + str(\", \\\"answer_end\\\": \") + str(end_index) + str(\"}\")))\n",
        "  \n",
        "# prepating the test answers in json format (with the answer's start/end positions in their contexts)\n",
        "test_answers = []\n",
        "for i in range(len(test_set)):\n",
        "  src_str = test_contexts[i]\n",
        "  start_index = src_str.find(test_set['Answer'][i])\n",
        "  end_index = start_index + len(test_set['Answer'][i])\n",
        "  test_answers.append(str(\"{\\\"text\\\": \\\"\" + test_set['Answer'][i] + str(\"\\\", \\\"answer_start\\\": \") + \n",
        "                           str(start_index) + str(\", \\\"answer_end\\\": \") + str(end_index) + str(\"}\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlvWY_npCfIm",
        "outputId": "7d28352c-82c7-4535-e232-09f7275d0598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Romania is a secular state, thus having no national religion. The dominant religious body is the Romanian Orthodox Church; its members make up 86.7% of the population according to the 2002 census. Other important religions include Roman Catholicism (4.7%), Protestantism (3.7%), Pentecostal denominations (1.5%) and the Romanian Greek-Catholic Church (0.9%).  Romania also has a historically significant Muslim minority concentrated in Dobrogea, mostly of Turkish ethnicity and numbering 67,500 people.     Romanian Census Website with population by religion  Based on the 2002 census data, there are also 6,179 Jews, 23,105 people who are of no religion and/or atheist, and 11,734 who refused to answer. On December 27, 2006, a new Law on Religion was approved under which religious denominations can only receive official registration if they have at least 20,000 members, or about 0.1 percent of Romania's total population.  Romania President Approves Europe's \"Worst Religion Law\"\n",
            "Why doesn't Romania have a state religion?  \n",
            "{\"text\": \"Romania is a secular state, thus having no national religion.\", \"answer_start\": 0, \"answer_end\": 61} \n",
            "\n",
            "According to the 2002 census, Romania has a population of 21,698,181 and, similarly to other countries in the region, is expected to gently decline in the coming years as a result of sub-replacement fertility rates. Romanians make up 89.5% of the population. The largest ethnic minorities are Hungarians, who make up 6.6% of the population and Roma, or Gypsies, who make up 2% of the population. By the official census 535,250 Roma live in Romania. 2002 census data, based on  Population by ethnicity, gives a total of 535,250 Roma in Romania. This figure is disputed by other sources, because at the local level, many Roma declare a different ethnicity (mostly Romanian, but also Hungarian in the West and Turkish in Dobruja) for fear of discrimination. Many are not recorded at all, since they   do not have ID cards. International sources give higher figures than the official census( UNDP's Regional Bureau for Europe,  World Bank,  International Association for Official Statistics).   usatoday: European effort spotlights plight of the Roma  Hungarians, who are a sizeable minority in Transylvania, constitute a majority in the counties of Harghita and Covasna. Ukrainians, Germans, Lipovans, Turks, Tatars, Serbs, Slovaks, Bulgarians, Croats, Greeks, Russians, Jews, Czechs, Poles, Italians, Armenians, as well as other ethnic groups, account for the remaining 1.4% of the population.  Official site of the results of the 2002 Census\n",
            "Is it true that romania has a population of 21,698,181?\n",
            "{\"text\": \"Romania has a population of 21,698,181\", \"answer_start\": 30, \"answer_end\": 68} \n",
            "\n",
            "In 1908, the year of his death, Becquerel was elected Permanent Secretary of the AcadÃ©mie des Sciences. He died at the age of 55 in Le Croisic.\n",
            "When did Becquerel die?\n",
            "{\"text\": \"In 1908\", \"answer_start\": 0, \"answer_end\": 7}\n"
          ]
        }
      ],
      "source": [
        "# visualizing/checking the context, question, and answer pairs\n",
        "i=40\n",
        "print(train_contexts[i])\n",
        "print(train_questions[i])\n",
        "print(train_answers[i], \"\\n\")\n",
        "\n",
        "i=9\n",
        "print(val_contexts[i])\n",
        "print(val_questions[i])\n",
        "print(val_answers[i], \"\\n\")\n",
        "\n",
        "i=0\n",
        "print(test_contexts[i])\n",
        "print(test_questions[i])\n",
        "print(test_answers[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUOACn8Ednbg",
        "outputId": "00174ba7-4b5b-496e-87f4-ab942609d886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Romania is a secular state, thus having no national religion. The dominant religious body is the Romanian Orthodox Church; its members make up 86.7% of the population according to the 2002 census. Other important religions include Roman Catholicism (4.7%), Protestantism (3.7%), Pentecostal denominations (1.5%) and the Romanian Greek-Catholic Church (0.9%).  Romania also has a historically significant Muslim minority concentrated in Dobrogea, mostly of Turkish ethnicity and numbering 67,500 people.     Romanian Census Website with population by religion  Based on the 2002 census data, there are also 6,179 Jews, 23,105 people who are of no religion and/or atheist, and 11,734 who refused to answer. On December 27, 2006, a new Law on Religion was approved under which religious denominations can only receive official registration if they have at least 20,000 members, or about 0.1 percent of Romania's total population.  Romania President Approves Europe's \"Worst Religion Law\"\n",
            "Is Romania a religious state?\n",
            "{\"text\": \"Romania is a secular state\", \"answer_start\": 0, \"answer_end\": 26}\n"
          ]
        }
      ],
      "source": [
        "i=6\n",
        "print(test_contexts[i])\n",
        "print(test_questions[i])\n",
        "print(test_answers[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4Wv_UIPTGJE6"
      },
      "outputs": [],
      "source": [
        "# initialize the tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "# tokenize\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_contexts, test_questions, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "0QB1Ec9yHM-6",
        "outputId": "f173914b-4f3b-4b8f-f17e-ecdd5e1d3f55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] a collision with a vehicle is capable of killing a kangaroo. kangaroos dazzled by headlights or startled by engine noise have been known to leap in front of cars. since kangaroos in mid - bound can reach speeds of around 50 km / h ( 31 mph ) and are relatively heavy, the force of impact can be severe. small vehicles may be destroyed, while larger vehicles may suffer engine damage. the risk of harm to vehicle occupants is greatly increased if the windscreen is the point of impact. as a result, \" kangaroo crossing \" signs are commonplace in australia. [SEP] is a collision with a vehicle capable of killing a kangaroo? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# checking one of the encoded inputs to observe the tokens\n",
        "tokenizer.decode(train_encodings['input_ids'][30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z7IM6lkrMLEx"
      },
      "outputs": [],
      "source": [
        "# transforming the answers from strings to json formats to access and use the values/info easily later\n",
        "for i in range(len(train_answers)):\n",
        "  train_answers[i]= json.loads(train_answers[i])\n",
        "\n",
        "for i in range(len(val_answers)):\n",
        "  val_answers[i]= json.loads(val_answers[i])\n",
        "\n",
        "for i in range(len(test_answers)):\n",
        "  test_answers[i]= json.loads(test_answers[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dZAVk_CuIDkv"
      },
      "outputs": [],
      "source": [
        "# defining a function to add the answers and the needed tokens to the encoded training and validation\n",
        "def add_token_positions(encodings, answers):\n",
        "    # initialize lists to contain the token indices of answer start/end\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        # append start/end token position using char_to_token method\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        # end position cannot be found, char_to_token found space, so shift position until found\n",
        "        shift = 1\n",
        "        while end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "    # update our encodings object with the new token-based start/end positions\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "# apply function to our data\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)\n",
        "add_token_positions(test_encodings, test_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsSIQqcbPO1C",
        "outputId": "7707e1f1-33fc-490a-b8f8-dfdab90ee093"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#checking the encoding keys\n",
        "train_encodings.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gtAj9V_tPaBG"
      },
      "outputs": [],
      "source": [
        "# defining a class to prepare and build the sets in the appropriate format for training\n",
        "class QA_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# build datasets for both our training and validation sets\n",
        "train_dataset = QA_dataset(train_encodings)\n",
        "val_dataset = QA_dataset(val_encodings)\n",
        "test_dataset = QA_dataset(test_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAGlByuLRh-t",
        "outputId": "24e2f541-23d1-45ca-c1d5-4a2ba2273b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# building the model\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6ZRrOenPt5c",
        "outputId": "056d22bf-e72c-49f1-8acd-3fc7a592ea9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Epoch 0: 100%|██████████| 5/5 [00:03<00:00,  1.48it/s, loss=5.06]\n",
            "Epoch 1: 100%|██████████| 5/5 [00:03<00:00,  1.48it/s, loss=4.09]\n",
            "Epoch 2: 100%|██████████| 5/5 [00:03<00:00,  1.47it/s, loss=3.12]\n",
            "Epoch 3: 100%|██████████| 5/5 [00:03<00:00,  1.49it/s, loss=2.57]\n",
            "Epoch 4: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, loss=1.84]\n",
            "Epoch 5: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, loss=1.53]\n",
            "Epoch 6: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, loss=1.22]\n",
            "Epoch 7: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, loss=0.933]\n",
            "Epoch 8: 100%|██████████| 5/5 [00:03<00:00,  1.49it/s, loss=0.79]\n",
            "Epoch 9: 100%|██████████| 5/5 [00:03<00:00,  1.49it/s, loss=0.647]\n",
            "Epoch 10: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, loss=0.661]\n",
            "Epoch 11: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, loss=0.635]\n",
            "Epoch 12: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, loss=0.406]\n"
          ]
        }
      ],
      "source": [
        "# setup GPU/CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# move model over to detected device\n",
        "model.to(device)\n",
        "# activate training mode of model\n",
        "model.train()\n",
        "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# initialize data loader for training data\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "losses=[]\n",
        "\n",
        "# start the training, with the number of epochs manipulated in the range\n",
        "for epoch in range(13):\n",
        "    # set model to train mode\n",
        "    model.train()\n",
        "    # setup loop (we use tqdm for the progress bar)\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all the tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        # train model on batch and return outputs (incl. loss)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "        # extract loss\n",
        "        loss = outputs[0]\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "        #save loss in a list to plot\n",
        "        losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "B4ytWmqtxz2m",
        "outputId": "cb65fc59-e5f6-4c26-bb34-27815da25d36"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(range(0,65), losses)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ZmECMwmk6s",
        "outputId": "b10d8df5-208c-4633-b12e-7737c927ad6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/models/tokenizer_config.json',\n",
              " '/content/models/special_tokens_map.json',\n",
              " '/content/models/vocab.txt',\n",
              " '/content/models/added_tokens.json',\n",
              " '/content/models/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#save the model in case it is needed later\n",
        "model_path = '/content/models'\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OEUgGPnCSx-c"
      },
      "outputs": [],
      "source": [
        "#load model for later use if needed\n",
        "#model_path = '/content/models'\n",
        "#model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
        "#tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OUiITGisfUT",
        "outputId": "c74a7639-8e04-42e5-98cd-ab0fafc56a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  4.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy=  85.00000238418579 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# switch model out of training mode\n",
        "model.eval()\n",
        "\n",
        "ds= val_dataset\n",
        "#ds= test_dataset\n",
        "set_loader = DataLoader(ds, batch_size=10)\n",
        "\n",
        "acc = []\n",
        "\n",
        "# initialize loop for progress bar\n",
        "loop = tqdm(set_loader)\n",
        "# loop through batches\n",
        "for batch in loop:\n",
        "    # we don't need to calculate gradients as we're not training\n",
        "    with torch.no_grad():\n",
        "        # pull batched items from loader\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_true = batch['start_positions'].to(device)\n",
        "        end_true = batch['end_positions'].to(device)\n",
        "        # make predictions\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        # pull preds out\n",
        "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
        "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
        "        # calculate accuracy for both and append to accuracy list\n",
        "        acc.append(((start_pred == start_true).sum()/len(start_pred)).item())\n",
        "        acc.append(((end_pred == end_true).sum()/len(end_pred)).item())\n",
        "\n",
        "# calculate average accuracy in total\n",
        "acc = sum(acc)/len(acc)\n",
        "print(\"\\nAccuracy= \", acc*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj1JIQUVLSZk",
        "outputId": "ea055419-f7b7-4b65-a21d-76b3c5fff74f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\ttype\tstart\tend\n",
            "\n",
            "0\ttrue\t14\t23\n",
            "\tpred\t14\t23\n",
            "\n",
            "1\ttrue\t13\t33\n",
            "\tpred\t13\t12\n",
            "\n",
            "2\ttrue\t22\t29\n",
            "\tpred\t22\t29\n",
            "\n",
            "3\ttrue\t1\t8\n",
            "\tpred\t1\t8\n",
            "\n",
            "4\ttrue\t206\t221\n",
            "\tpred\t206\t221\n",
            "\n",
            "5\ttrue\t38\t44\n",
            "\tpred\t38\t44\n",
            "\n",
            "6\ttrue\t1\t8\n",
            "\tpred\t1\t8\n",
            "\n",
            "7\ttrue\t73\t80\n",
            "\tpred\t73\t31\n",
            "\n",
            "8\ttrue\t4\t16\n",
            "\tpred\t1\t16\n",
            "\n",
            "9\ttrue\t7\t17\n",
            "\tpred\t7\t17\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"#\\ttype\\tstart\\tend\\n\")\n",
        "for i in range(len(start_true)):\n",
        "    print(f\"{i}\\ttrue\\t{start_true[i]}\\t{end_true[i]}\\n\"\n",
        "          f\"\\tpred\\t{start_pred[i]}\\t{end_pred[i]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x2JhkZoPpbI",
        "outputId": "3ada2d86-766b-4cba-fab1-644e83642b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question ( 0 ):  What happened in 1896?\n",
            "True answer:  Becquerel accidentally discovered radioactivity.\n",
            "Predicted answer:\n",
            "becquerel accidentally discovered radioactivity\n",
            "\n",
            "Question ( 1 ):  Have Kangaroos dazzled by headlights or startled by engine noise been known to leap in front of cars ?\n",
            "True answer:  Kangaroos dazzled by headlights or startled by engine noise have been known to leap in front of cars.\n",
            "Predicted answer:\n",
            "\n",
            "\n",
            "Question ( 2 ):  Is the president elected by popular vote?\n",
            "True answer:  The president is elected by popular vote\n",
            "Predicted answer:\n",
            "the president is elected by popular vote\n",
            "\n",
            "Question ( 3 ):  Is romania a semi-presidential unitary state?\n",
            "True answer:  Romania is a semi-presidential democratic republic\n",
            "Predicted answer:\n",
            "romania is a semi - presidential democratic\n",
            "\n",
            "Question ( 4 ):  What may be called Volta's Law of the electrochemical series?\n",
            "True answer:  two identical electrodes and a common electrolyte give zero net emf\n",
            "Predicted answer:\n",
            "two identical electrodes and a common electrolyte give zero net emf\n",
            "\n",
            "Question ( 5 ):  Where do joeys complete postnatal development?\n",
            "True answer:  a pouch called a marsupium\n",
            "Predicted answer:\n",
            "a pouch called a marsup\n",
            "\n",
            "Question ( 6 ):  How many counties is Romania divided into?\n",
            "True answer:  Romania is divided into forty-one counties\n",
            "Predicted answer:\n",
            "romania is divided into forty - one\n",
            "\n",
            "Question ( 7 ):  Was his 1800 paper written in French ?\n",
            "True answer:  His 1800 paper was written in French.\n",
            "Predicted answer:\n",
            "\n",
            "\n",
            "Question ( 8 ):  Is it true that he shared the nobel prize in physics?\n",
            "True answer:  he shared the Nobel Prize in Physics with Pierre and Marie Curie\n",
            "Predicted answer:\n",
            "in 1903, he shared the nobel prize in physics with pierre and marie cu\n",
            "\n",
            "Question ( 9 ):  Is it true that romania has a population of 21,698,181?\n",
            "True answer:  Romania has a population of 21,698,181\n",
            "Predicted answer:\n",
            "romania has a population of 21, 698,\n"
          ]
        }
      ],
      "source": [
        "if ds == test_dataset:\n",
        "  for i in range(len(start_true)):\n",
        "    print(\"\\nQuestion (\", i, \"): \", test_questions[i])\n",
        "    print(\"True answer: \", test_answers[i]['text'])\n",
        "    print(\"Predicted answer:\")\n",
        "    answer = tokenizer.decode(batch['input_ids'][i][start_pred[i] : end_pred[i]])\n",
        "    print(answer)\n",
        "\n",
        "if ds == val_dataset:\n",
        "  for i in range(len(start_true)):\n",
        "    print(\"\\nQuestion (\", i, \"): \", val_questions[i])\n",
        "    print(\"True answer: \", val_answers[i]['text'])\n",
        "    print(\"Predicted answer:\")\n",
        "    answer = tokenizer.decode(batch['input_ids'][i][start_pred[i] : end_pred[i]])\n",
        "    print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "788 Project - Deep Learning BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}