{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "788_Project_Chatbot_NLP_BasicNLTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-XPjA4Tjm-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115d988a-1cd2-4938-d12c-a3331dc20b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mouting google drive to import data saved on it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import nltk\n",
        "import random\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "metadata": {
        "id": "AX9E9MzPHgP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b79835-c63b-4641-8313-798c5251a2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Opening the different sets of data\n",
        "s08 = pd.read_csv('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt', sep='\\t', encoding = 'ISO-8859-1')\n",
        "s09 = pd.read_csv('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S09/question_answer_pairs.txt', sep='\\t', encoding = 'ISO-8859-1')\n",
        "s10 = pd.read_csv('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S10/question_answer_pairs.txt', sep='\\t', encoding = 'ISO-8859-1')"
      ],
      "metadata": {
        "id": "Pd7YvB-DHo6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a new variable to manipulate the data while keeping the original\n",
        "s08_new = s08\n",
        "s09_new = s09\n",
        "s10_new = s10"
      ],
      "metadata": {
        "id": "gD77_eOfr5iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping columns which we will not use and removing any NAN values then resetting the index for each set of data\n",
        "s08_new = s08_new.drop(columns = ['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleTitle'])\n",
        "s08_new = s08_new.dropna()\n",
        "s08_new = s08_new.reset_index(drop=True)\n",
        "\n",
        "s09_new = s09_new.drop(columns = ['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleTitle'])\n",
        "s09_new = s09_new.dropna()\n",
        "s09_new = s09_new.reset_index(drop=True)\n",
        "\n",
        "s10_new = s10_new.drop(columns = ['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleTitle'])\n",
        "s10_new = s10_new.dropna()\n",
        "s10_new = s10_new.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "kbM0V9iNsBal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the article and storing it by using the path instead then replacing all \\n to a space\n",
        "for i in range(len(s08_new)):\n",
        "    article = open('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S08/' + str(s08_new['ArticleFile'].loc[i]) + '.txt.clean', encoding = 'ISO-8859-1')\n",
        "    s08_new['ArticleFile'].loc[i] = (article).read()\n",
        "    article.close()\n",
        "\n",
        "for i in range(len(s09_new)):\n",
        "    article = open('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S09/' + str(s09_new['ArticleFile'].loc[i]) + '.txt.clean', encoding = 'ISO-8859-1')\n",
        "    s09_new['ArticleFile'].loc[i] = (article).read()\n",
        "    article.close()\n",
        "\n",
        "for i in range(len(s10_new)):\n",
        "    article = open('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S10/' + str(s10_new['ArticleFile'].loc[i]) + '.txt.clean', encoding = 'ISO-8859-1')\n",
        "    s10_new['ArticleFile'].loc[i] = (article).read()\n",
        "    article.close()\n",
        "\n",
        "s08_new = s08_new.replace('\\n',' ', regex=True)\n",
        "s09_new = s09_new.replace('\\n',' ', regex=True)\n",
        "s10_new = s10_new.replace('\\n',' ', regex=True)"
      ],
      "metadata": {
        "id": "u9IxC8znuRza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the questions into train and validation sets\n",
        "\n",
        "s08_val = s08_new.sample(frac=0.1, replace=False, axis=0, ignore_index=False)"
      ],
      "metadata": {
        "id": "pMjwH5lv3KNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining all Articles into one variable\n",
        "\n",
        "Combined_data = \"\"\n",
        "\n",
        "# Getting the S08 articles into combined data\n",
        "for i in range(4):\n",
        "  for y in range(10):\n",
        "    open_data = open('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S08/data/set' + str(i+1) + '/a' + str(y+1) + '.txt.clean', 'r', errors = 'ignore')\n",
        "    raw_data = open_data.read()\n",
        "    raw_data = raw_data.lower()\n",
        "    Combined_data = Combined_data + raw_data\n",
        "\n",
        "# Getting the S09 articles into combined data\n",
        "for i in range(5):\n",
        "  for y in range(10):\n",
        "    open_data = open('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S09/data/set' + str(i+1) + '/a' + str(y+1) + '.txt.clean', 'r', errors = 'ignore')\n",
        "    raw_data = open_data.read()\n",
        "    raw_data = raw_data.lower()\n",
        "    Combined_data = Combined_data + raw_data\n",
        "\n",
        "# Getting the S10 articles into combined data\n",
        "for i in range(6):\n",
        "  for y in range(10):\n",
        "    open_data = open('/content/drive/MyDrive/Question_Answer_Dataset_v1.2/S10/data/set' + str(i+1) + '/a' + str(y+1) + '.txt.clean', 'r', errors = 'ignore')\n",
        "    raw_data = open_data.read()\n",
        "    raw_data = raw_data.lower()\n",
        "    Combined_data = Combined_data + raw_data"
      ],
      "metadata": {
        "id": "fXDtF83P5Etk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the Data into sentences and words\n",
        "\n",
        "sent_token = nltk.sent_tokenize(Combined_data)\n",
        "word_token = nltk.word_tokenize(Combined_data)"
      ],
      "metadata": {
        "id": "1SfEzmBnQ-W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing the data (lemmatizing)\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "aM8qrUr-RLOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating some greeting inputs\n",
        "\n",
        "greet_input = (\"hi\", \"hello\", \"hey\", \"greetings\", \"hi there\", \"whats up\")\n",
        "greet_response = (\"hi\", \"hi there\", \"hello there\", \"Nice to meet you\", \"hey\", \"Hey I'm 788 Chatbot\")"
      ],
      "metadata": {
        "id": "GbD8XgozvnDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if user typed a greeting\n",
        "\n",
        "def greeting(user_input):\n",
        "  for word in user_input.split():\n",
        "    if word.lower() in greet_input:\n",
        "      return random.choice(greet_response)"
      ],
      "metadata": {
        "id": "tD-cv2Jfv2te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the chatbot respond to questions\n",
        "\n",
        "def response(user_response):\n",
        "  chatbot_response=''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_token)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf == 0):\n",
        "    chatbot_response = chatbot_response + \"I don't understand\"\n",
        "    return chatbot_response\n",
        "  else:\n",
        "    chatbot_response = chatbot_response + sent_token[idx]\n",
        "    return chatbot_response"
      ],
      "metadata": {
        "id": "kxW4vFAgv4zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the Chat bot and making it respond to questions\n",
        "\n",
        "flag = True\n",
        "print(\"788 Chatbot: My name is 788 Chatbot. I will answer questions about different Wikipedia Articles. To exit, type exit\")\n",
        "\n",
        "while(flag == True):\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "    if(user_response!='exit'):\n",
        "      if(user_response == 'thank you' or user_response == 'thanks'):\n",
        "        flag = False\n",
        "        print(\"788 Chatbot: You're welcome\")\n",
        "      else:\n",
        "        if(greeting(user_response)!=None):\n",
        "          print(\"788 Chatbot: \"+greeting(user_response))\n",
        "        else:\n",
        "          sent_token.append(user_response)\n",
        "          word_token = word_token + nltk.word_tokenize(user_response)\n",
        "          final_words = list(set(word_token))\n",
        "          print(\"788 Chatbot: \", end=\"\")\n",
        "          print(response(user_response))\n",
        "          sent_token.remove(user_response)\n",
        "    else:\n",
        "      flag = False\n",
        "      print(\"788 Chatbot: Bye\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIbxH7WUwOVG",
        "outputId": "13e62911-9031-44aa-fd81-d35e0fbb8725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: My name is 788 Chatbot. I will answer questions about different Wikipedia Articles. To exit, type exit\n",
            "abraham lincoln\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the assassination of abraham lincoln.\n",
            "who is abraham lincoln\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the assassination of abraham lincoln.\n",
            "elephant\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are three living species: the african bush elephant, the african forest elephant (until recently known collectively as the african elephant), and the asian elephant (also known as the indian elephant).\n",
            "when was abraham lincoln born\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the assassination of abraham lincoln.\n",
            "abraham lincoln was born\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the assassination of abraham lincoln.\n",
            "Thomas Lincoln and Nancy Hanks\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abraham lincoln was born on february 12, 1809, to thomas lincoln and nancy hanks, two uneducated farmers.\n",
            "who was thomas lincoln\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abraham lincoln was born on february 12, 1809, to thomas lincoln and nancy hanks, two uneducated farmers.\n",
            "what was lincoln's formal education\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.\n",
            "\n",
            "lincoln's formal education consisted of about 18 months of schooling.\n",
            "when did lincoln begin his political career\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":    \"he did it.\"\n",
            "Ulysses S. Grant\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ulysses s. grant\n",
            "\n",
            "  \n",
            "ulysses s. grant, see military career for a discussion of grant's middle initial.\n",
            "Hiram Ulysses Grant\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grant wrote his name in the entrance register as \"ulysses hiram grant\" (concerned that he would otherwise become known by his initials, h.u.g.\n",
            "hi\n",
            "Chatbot: hi\n",
            "hi\n",
            "Chatbot: hi there\n",
            "hi\n",
            "Chatbot: Hey I'm 788 Chatbot\n",
            "hi\n",
            "Chatbot: hey\n",
            "hi\n",
            "Chatbot: hello there\n",
            "hi\n",
            "Chatbot: hey\n",
            "hi\n",
            "Chatbot: Hey I'm 788 Chatbot\n",
            "i\n",
            "Chatbot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't understand\n",
            "exit\n",
            "Chatbot: Bye\n"
          ]
        }
      ]
    }
  ]
}